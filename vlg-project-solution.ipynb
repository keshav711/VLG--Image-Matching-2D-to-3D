{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\"Based on Imc 23 sample submission \"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install virtualenv -q\n!virtualenv myenv \n!myenv/Scripts/activate","metadata":{"execution":{"iopub.status.busy":"2023-06-23T03:08:43.490236Z","iopub.execute_input":"2023-06-23T03:08:43.491442Z","iopub.status.idle":"2023-06-23T03:08:58.103387Z","shell.execute_reply.started":"2023-06-23T03:08:43.491392Z","shell.execute_reply":"2023-06-23T03:08:58.101960Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mcreated virtual environment CPython3.7.12.final.0-64 in 738ms\n  creator CPython3Posix(dest=/kaggle/working/myenv, clear=False, no_vcs_ignore=False, global=False)\n  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n    added seed packages: pip==22.3.1, setuptools==65.6.3, wheel==0.38.4\n  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n/bin/bash: myenv/Scripts/activate: No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"#Pandas\ntry:\n    import pandas as pd \n    print(\"pandas version-\",pd.__version__)\nexcept:\n    !pip install pandas -q\n    import pandas as pd \n    print(\"pandas version-\",pd.__version__)\n#numpy\ntry:\n    import numpy as np\n    print(\"numpy version-\",np.__version__)\nexcept:\n    !pip install numpy -q\n    import numpy as np\n    print(\"numpy version-\",np.__version__)\n#os,collections,time,path,garbage,tqdm\nimport os\nfrom collections import defaultdict\nimport time\nimport gc\nimport tqdm\nfrom pathlib import Path\nimport csv\nfrom copy import *\nimport math\nimport numbers\nimport random\n\"\"\"\"#progress bar\ntry:\n    from fastprogress import progessbar\nexcept:\n    !pip install fastprogess\n    from fastprogress import progessbar\"\"\"\n#opencv   \ntry:\n    import cv2\n    print(\"opencv version-\",cv2.__version__)\nexcept:\n    !pip install opencv-python -q\n    import cv2\n    print(\"opencv version\",cv2.__version__)\n#scikit-image\ntry:\n    import skimage\n    print(\"scikit image version\",skimage.__version__) \nexcept:\n    !pip install scikit-image -q\n    import skimage\n    print(\"scikit image version\",skimage.__version__)\n#PIL Image\ntry:\n    from PIL import Image,ExifTags\nexcept:\n    !pip install PIL\n    from PIL import Image,ExifTags\n#pycolmap    \ntry:\n    import pycolmap\n    print(\"pycolmap version\",pycolmap.__version__)\nexcept:\n    !pip install pycolmap -q\n    import pycolmap\n    print(\"pycolmap version\",pycolmap.__version__)\n#mediapy\ntry:\n    import mediapy as media\n    print(\"mediapy version\", media.__version__)\nexcept: \n    !pip install mediapy -q\n    import mediapy as media\n    print(\"mediapy version\", media.__version__)\n#glob\nfrom glob import glob\n\n#h5py for hdf5 format\ntry:\n    import h5py\nexcept: \n    !pip install h5py -q\n    import h5py\n#pytorch & nn functions\ntry:\n    import torch \n    import torch.nn.functional as F\n    device=torch.device('cuda')\n    print(\"torch version\", torch.__version__)\nexcept: \n    !pip install torch -q\n    import torch\n    import torch.nn.functionaal as F\n    device=torch.device('cuda')\n    print(\"torch version\", torch.__version__)\n#TIMM\ntry: \n    import timm\n    from timm.data import resolve_data_config\n    from timm.data.auto_augment import rand_augment_transform, augment_and_mix_transform, auto_augment_transform\n    from timm.data.random_erasing import RandomErasing\n    from timm.data.transforms_factory import create_transform\nexcept:\n    !pip install timm\n    import timm\n    from timm.data import resolve_data_config\n    from timm.data.auto_augment import rand_augment_transform, augment_and_mix_transform, auto_augment_transform\n    from timm.data.random_erasing import RandomErasing\n    from timm.data.transforms_factory import create_transform\n#Kornia\ntry:\n    import kornia \n    print(\"kornia version\", kornia.__version__)\nexcept: \n    !pip install kornia -q\n    import kornia\n    print(\"kornia version\", kornia.__version__)\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T06:25:21.934440Z","iopub.execute_input":"2023-06-23T06:25:21.935214Z","iopub.status.idle":"2023-06-23T06:25:42.017289Z","shell.execute_reply.started":"2023-06-23T06:25:21.935175Z","shell.execute_reply":"2023-06-23T06:25:42.015397Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"pandas version- 1.3.5\nnumpy version- 1.21.6\nopencv version- 4.5.4\nscikit image version 0.19.3\npycolmap version 0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mmediapy version 1.1.2\ntorch version 1.13.0\nkornia version 0.6.11\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# LOCAL_FEATURE = 'DISK'\nLOCAL_FEATURE = 'KeyNetAffNetHardNet'\n# LOCAL_FEATURE = 'LoFTR'\n\n# Explanation:\n# - Uncomment the desired local feature extraction method by assigning it to the LOCAL_FEATURE variable.\n# - Each method represents a different approach for extracting local features in computer vision tasks.\n\n# Option 1: DISK\n# - DISK (Dense Invariant Spatial Keypoints) is a method that detects dense keypoints across an image and computes descriptors for them.\n# - These keypoints are invariant to certain image transformations like scale and rotation.\n\n# Option 2: KeyNetAffNetHardNet\n# - KeyNetAffNetHardNet is a combination of three components: KeyNet, AffNet, and HardNet.\n# - KeyNet detects keypoints in an image, AffNet performs affine shape adaptation on the keypoints, and HardNet computes descriptors for the adapted keypoints.\n# - This method provides robust local feature extraction.\n\n# Option 3: LoFTR\n# - LoFTR (Local Features for Text Recognition) is a method specifically designed for text recognition tasks.\n# - It focuses on extracting and matching local features in text images to enable accurate text recognition.\n\n# By selecting one of these options and assigning it to the LOCAL_FEATURE variable,\n# you can choose the desired local feature extraction method for your computer vision task.\n# Make sure to uncomment the desired option and comment out the others accordingly.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-23T06:25:42.019303Z","iopub.execute_input":"2023-06-23T06:25:42.019625Z","iopub.status.idle":"2023-06-23T06:25:42.026580Z","shell.execute_reply.started":"2023-06-23T06:25:42.019593Z","shell.execute_reply":"2023-06-23T06:25:42.025341Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"min_pairs = 50 #20\nnum_features = 8192 #2048\nresize_small_edge_to = 1200 #600\nsim_th = 0.3\nAPPLY_CLAHE = True","metadata":{"execution":{"iopub.status.busy":"2023-06-23T06:25:42.028419Z","iopub.execute_input":"2023-06-23T06:25:42.029212Z","iopub.status.idle":"2023-06-23T06:25:42.045189Z","shell.execute_reply.started":"2023-06-23T06:25:42.029159Z","shell.execute_reply":"2023-06-23T06:25:42.043867Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def to_str(a):\n    return ';'.join([str(x) for x in a.reshape(-1)])\n\n\ndef convert_clahe(initial):\n    \"\"\"\n    Convert an image using Contrast Limited Adaptive Histogram Equalization (CLAHE).\n\n    Parameters:\n        initial (ndarray): The input image as a NumPy array.\n\n    Returns:\n        ndarray: The CLAHE-enhanced version of the input image.\n\n    Description:\n        This function applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to enhance the local contrast\n        of an input image. The CLAHE algorithm redistributes pixel intensities in a way that improves the visibility\n        of local details while preventing excessive contrast enhancement.\n\n        The input image should be in BGR color format. The function performs the following steps:\n\n        1. Create a CLAHE object with a clipLimit of 2.0 and a tileGridSize of (8,8).\n        2. Convert the input image to the HSV color space.\n        3. Extract the intensity (value) channel from the HSV image.\n        4. Apply the CLAHE algorithm to the intensity channel.\n        5. Convert the image back to the BGR color space.\n        6. Return the CLAHE-enhanced image.\n\n    Example:\n        # Load an image\n        image = cv2.imread('input.jpg')\n\n        # Apply CLAHE conversion\n        clahe_image = convert_clahe(image)\n\n        # Display the result\n        cv2.imshow('CLAHE-enhanced Image', clahe_image)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n    \"\"\"\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    ac = np.asarray(initial)\n    ac = cv2.cvtColor(ac, cv2.COLOR_BGR2HSV)\n    ac[:,:,-1] = clahe.apply(ac[:,:,-1])\n    ac = cv2.cvtColor(ac, cv2.COLOR_HSV2BGR)\n    return ac\n\n\ndef imageof(file_name, device=torch.device('cpu')):\n    \"\"\"\n    Load an image file using OpenCV and convert it to a torch tensor.\n\n    Parameters:\n        file_name (str): The path to the image file.\n        device (torch.device, optional): The device to which the torch tensor will be moved. Defaults to 'cpu'.\n\n    Returns:\n        torch.Tensor: The loaded image as a torch tensor.\n\n    Description:\n        This function loads an image file using OpenCV and converts it to a torch tensor, which is suitable for further\n        processing in a PyTorch-based model.\n\n        If the `APPLY_CLAHE` flag is set to `True`, the function applies Contrast Limited Adaptive Histogram Equalization\n        (CLAHE) to enhance the local contrast of the image before conversion. Otherwise, the image is loaded without any\n        additional preprocessing.\n\n        The image is loaded as a BGR image using OpenCV's `cv2.imread()` function. It is then converted to a torch tensor\n        and normalized by dividing by 255. The image is further converted from the BGR color space to the RGB color space\n        using Kornia's `color.bgr_to_rgb()` function.\n\n        Finally, the resulting torch tensor is moved to the specified device before being returned.\n\n    Example:\n        # Load and preprocess an image\n        file_path = 'image.jpg'\n        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        image = load_torch(file_path, device)\n\n        # Perform further processing using the torch tensor\n        output = model(image)\n    \"\"\"\n    if APPLY_CLAHE:\n        image_ = convert_clahe(cv2.imread(file_name))\n    else:\n        image_ = cv2.imread(file_name)\n\n    image_ = K.image_to_tensor(image_, False).float() / 255.\n    image_ = K.color.bgr_to_rgb(image_.to(device))\n    return image_\n","metadata":{"execution":{"iopub.status.busy":"2023-06-23T06:30:40.361314Z","iopub.execute_input":"2023-06-23T06:30:40.362089Z","iopub.status.idle":"2023-06-23T06:30:40.373849Z","shell.execute_reply.started":"2023-06-23T06:30:40.362047Z","shell.execute_reply":"2023-06-23T06:30:40.372490Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"The purpose of the glob_inf function is to extract global information features from a set of images using the provided model. Here's a breakdown of the code:\n\n1.The model is set to evaluation mode and moved to the specified device.\n2.The config dictionary is resolved using the resolve_data_config function, providing an empty dictionary and the model as arguments.\n3.The transform is created using the create_transform function with the config as arguments.\n4.An empty list glob_info_next is initialized to store the extracted global information features.\n5.The function iterates over each image file name in the file_names list using tqdm for progress tracking.\n6.The base name of the image file is extracted using os.path.basename and split into the key and extension using os.path.splitext.\n7.The image is opened using Image.open and converted to RGB format.\n8.The image is transformed using the defined transform.\n9.The transformed image is unsqueezed to add a batch dimension and moved to the specified device.\n10.The model is used to extract features from the transformed image using model.forward_features.\n11.The extracted features are averaged over the width and height dimensions using .mean(dim=(-1, 2)).\n12.The resulting features are reshaped to have a size of (1, -1).\n13.The features are normalized along dimension 1 using F.normalize and the L2 norm.\n14.The normalized features are detached from the computational graph and moved to the CPU.\n15.The normalized features are appended to the glob_info_next list.\n16.The global_info_ tensor is created by concatenating the features in glob_info_next along dimension 0.\n17.The global_info_ tensor containing the global information features is returned.\n18.The function essentially performs feature extraction on a set of images using a given model and returns the concatenated global information features.\n\n\n\n\n\n\"\"\"\ndef glob_inf(file_names, model,\n                    device =  torch.device('cpu')):\n    model = model.eval()\n    model= model.to(device)\n    config = resolve_data_config({}, model=model)\n    transform = create_transform(**config)\n    glob_info_next=[]\n    for i, img_name in tqdm(enumerate(file_names),total= len(file_names)):\n        key = os.path.splitext(os.path.basename(img_name))[0]\n        image_ = Image.open(img_name).convert('RGB')\n        new_img = transform(image_).unsqueeze(0).to(device)\n        with torch.no_grad():\n            info = model.forward_features(new_img.to(device)).mean(dim=(-1,2))#\n            #print (desc.shape)\n            info = info.view(1, -1)\n            info_normal = F.normalize(ifno, dim=1, p=2)\n        global_info_next.append(info_normal.detach().cpu())\n    global_info_ = torch.cat(global_info_next, dim=0)\n    return global_info_\n\n\"\"\"pairs(file_names):\n\nParameters:\nfile_names: A list of file names representing images.\nReturns:\npairs: A list of pairs representing unique combinations of indices from the file_names list.\nDescription:\nThis function generates pairs of indices from the file_names list. Each pair consists of two indices (i, j) where i and j are different and range from 0 to len(file_names) - 1. The function iterates over the range of indices and appends each unique pair to the pairs list. The resulting pairs list is returned.\npairs_shortlist(file_names, sim_thresh, min_pairs, exhaustiveless, device=torch.device('cpu')):\n\nParameters:\nfile_names: A list of file names representing images.\nsim_thresh: A similarity threshold value (float) used for pair selection.\nmin_pairs: The minimum number of pairs to be generated.\nexhaustiveless: An integer value specifying the maximum number of images for which exhaustive pair generation will be performed. If the number of images is less than or equal to exhaustiveless, the function falls back to the pairs function.\ndevice: An optional parameter specifying the device on which the model should be loaded. By default, it is set to torch.device('cpu').\nReturns:\nmatch_list: A list of pairs representing shortlisted combinations of indices from the file_names list.\nDescription:\nThis function generates a shortlist of pairs from the file_names list based on similarity criteria. If the number of images is less than or equal to exhaustiveless, the function directly calls the pairs function to generate pairs. Otherwise, it performs the following steps:\nIt loads a pre-trained EfficientNet-B7 model using timm.create_model.\nThe model is set to evaluation mode.\nGlobal information features are extracted from the images using the global_inf function.\nPairwise distances between the extracted features are computed using torch.cdist.\nA mask is created based on the similarity threshold.\nFor each image, matches are obtained based on the mask and minimum pairs requirement.\nThe resulting matches are filtered and appended to the match_list.\nThe match_list is sorted and returned as the final shortlisted pairs.\n\"\"\"\n\n\ndef pairs(file_names):\n    pairs = []\n    for i in range(len(file_names)):\n        for j in range(i+1, len(file_names)):\n            pairs.append((i,j))\n    return pairs\n\n\ndef pairs_shortlist(file_names,\n                              sim_thresh = 0.6, # should be strict\n                              min_pairs = 20,\n                              exhaustiveless = 20,\n                              device=torch.device('cpu')):\n    num = len(file_names)\n\n    if num <= exhaustiveless:\n        return pairs(file_names)\n\n    model = timm.create_model('tf_efficientnet_b7',\n                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n    model.eval()\n    infos = global_inf(file_names, model, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n    mask = dm <= sim_thresh    \n    total = 0\n    match_list = []\n    ar = np.arange(num)\n    already_there = []\n    for st in range(num-1):\n        mask = mask[st]\n        match = ar[mask]\n        if len(match) < min_pairs:\n            match = np.argsort(dm[st])[:min_pairs]  \n        for i in match:\n            if st == i:\n                continue\n            if dm[st, i] < 1000:\n                match_list.append(tuple(sorted((st, i.item()))))\n                total+=1\n    match_list = sorted(list(set(match_list)))\n    return match_list","metadata":{"execution":{"iopub.status.busy":"2023-06-23T06:44:18.094382Z","iopub.execute_input":"2023-06-23T06:44:18.094809Z","iopub.status.idle":"2023-06-23T06:44:18.113854Z","shell.execute_reply.started":"2023-06-23T06:44:18.094769Z","shell.execute_reply":"2023-06-23T06:44:18.112380Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# This script is based on an original implementation by True Price.\n\nimport sys\nimport sqlite3\nimport numpy as np\n\n\nIS_PYTHON3 = sys.version_info[0] >= 3\n\nMAX_IMAGE_ID = 2**31 - 1\n\nCREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    model INTEGER NOT NULL,\n    width INTEGER NOT NULL,\n    height INTEGER NOT NULL,\n    params BLOB,\n    prior_focal_length INTEGER NOT NULL)\"\"\"\n\nCREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n\nCREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    name TEXT NOT NULL UNIQUE,\n    camera_id INTEGER NOT NULL,\n    prior_qw REAL,\n    prior_qx REAL,\n    prior_qy REAL,\n    prior_qz REAL,\n    prior_tx REAL,\n    prior_ty REAL,\n    prior_tz REAL,\n    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n\"\"\".format(MAX_IMAGE_ID)\n\nCREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\nCREATE TABLE IF NOT EXISTS two_view_geometries (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    config INTEGER NOT NULL,\n    F BLOB,\n    E BLOB,\n    H BLOB)\n\"\"\"\n\nCREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n\"\"\"\n\nCREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB)\"\"\"\n\nCREATE_NAME_INDEX = \\\n    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n\nCREATE_ALL = \"; \".join([\n    CREATE_CAMERAS_TABLE,\n    CREATE_IMAGES_TABLE,\n    CREATE_KEYPOINTS_TABLE,\n    CREATE_DESCRIPTORS_TABLE,\n    CREATE_MATCHES_TABLE,\n    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n    CREATE_NAME_INDEX\n])\n\n\ndef image_ids_to_pair_id(image_id1, image_id2):\n    if image_id1 > image_id2:\n        image_id1, image_id2 = image_id2, image_id1\n    return image_id1 * MAX_IMAGE_ID + image_id2\n\n\ndef pair_id_to_image_ids(pair_id):\n    image_id2 = pair_id % MAX_IMAGE_ID\n    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n    return image_id1, image_id2\n\n\ndef array_to_blob(array):\n    if IS_PYTHON3:\n        return array.tostring()\n    else:\n        return np.getbuffer(array)\n\n\ndef blob_to_array(blob, dtype, shape=(-1,)):\n    if IS_PYTHON3:\n        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n    else:\n        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n\n\nclass COLMAPDatabase(sqlite3.Connection):\n\n    @staticmethod\n    def connect(database_path):\n        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n\n\n    def __init__(self, *args, **kwargs):\n        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n\n        self.create_tables = lambda: self.executescript(CREATE_ALL)\n        self.create_cameras_table = \\\n            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n        self.create_descriptors_table = \\\n            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n        self.create_images_table = \\\n            lambda: self.executescript(CREATE_IMAGES_TABLE)\n        self.create_two_view_geometries_table = \\\n            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n        self.create_keypoints_table = \\\n            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n        self.create_matches_table = \\\n            lambda: self.executescript(CREATE_MATCHES_TABLE)\n        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n\n    def add_camera(self, model, width, height, params,\n                   prior_focal_length=False, camera_id=None):\n        params = np.asarray(params, np.float64)\n        cursor = self.execute(\n            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n            (camera_id, model, width, height, array_to_blob(params),\n             prior_focal_length))\n        return cursor.lastrowid\n\n    def add_image(self, name, camera_id,\n                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n        cursor = self.execute(\n            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n        return cursor.lastrowid\n\n    def add_keypoints(self, image_id, keypoints):\n        assert(len(keypoints.shape) == 2)\n        assert(keypoints.shape[1] in [2, 4, 6])\n\n        keypoints = np.asarray(keypoints, np.float32)\n        self.execute(\n            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n\n    def add_descriptors(self, image_id, descriptors):\n        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n        self.execute(\n            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n\n    def add_matches(self, image_id1, image_id2, matches):\n        assert(len(matches.shape) == 2)\n        assert(matches.shape[1] == 2)\n\n        if image_id1 > image_id2:\n            matches = matches[:,::-1]\n\n        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n        matches = np.asarray(matches, np.uint32)\n        self.execute(\n            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n            (pair_id,) + matches.shape + (array_to_blob(matches),))\n\n    def add_two_view_geometry(self, image_id1, image_id2, matches,\n                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n        assert(len(matches.shape) == 2)\n        assert(matches.shape[1] == 2)\n\n        if image_id1 > image_id2:\n            matches = matches[:,::-1]\n\n        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n        matches = np.asarray(matches, np.uint32)\n        F = np.asarray(F, dtype=np.float64)\n        E = np.asarray(E, dtype=np.float64)\n        H = np.asarray(H, dtype=np.float64)\n        self.execute(\n            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n             array_to_blob(F), array_to_blob(E), array_to_blob(H)))","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:19.034288Z","iopub.execute_input":"2023-06-23T02:23:19.036356Z","iopub.status.idle":"2023-06-23T02:23:19.072024Z","shell.execute_reply.started":"2023-06-23T02:23:19.036292Z","shell.execute_reply":"2023-06-23T02:23:19.069533Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Code to interface DISK with Colmap.\n# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n\nimport os, argparse, h5py, warnings\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image, ExifTags\n\n\ndef get_focal(image_path, err_on_default=False):\n    image         = Image.open(image_path)\n    max_size      = max(image.size)\n\n    exif = image.getexif()\n    focal = None\n    if exif is not None:\n        focal_35mm = None\n        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n        for tag, value in exif.items():\n            focal_35mm = None\n            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n                focal_35mm = float(value)\n                break\n\n        if focal_35mm is not None:\n            focal = focal_35mm / 35. * max_size\n    \n    if focal is None:\n        if err_on_default:\n            raise RuntimeError(\"Failed to find focal length\")\n\n        # failed to find it in exif, use prior\n        FOCAL_PRIOR = 1.2\n        focal = FOCAL_PRIOR * max_size\n\n    return focal\n\ndef create_camera(db, image_path, camera_model):\n    image         = Image.open(image_path)\n    width, height = image.size\n\n    focal = get_focal(image_path)\n\n    if camera_model == 'simple-pinhole':\n        model = 0 # simple pinhole\n        param_arr = np.array([focal, width / 2, height / 2])\n    if camera_model == 'pinhole':\n        model = 1 # pinhole\n        param_arr = np.array([focal, focal, width / 2, height / 2])\n    elif camera_model == 'simple-radial':\n        model = 2 # simple radial\n        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n    elif camera_model == 'opencv':\n        model = 4 # opencv\n        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n         \n    return db.add_camera(model, width, height, param_arr)\n\n\ndef add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n\n    camera_id = None\n    fname_to_id = {}\n    for filename in tqdm(list(keypoint_f.keys())):\n        keypoints = keypoint_f[filename][()]\n\n        fname_with_ext = filename# + img_ext\n        path = os.path.join(image_path, fname_with_ext)\n        if not os.path.isfile(path):\n            raise IOError(f'Invalid image path {path}')\n\n        if camera_id is None or not single_camera:\n            camera_id = create_camera(db, path, camera_model)\n        image_id = db.add_image(fname_with_ext, camera_id)\n        fname_to_id[filename] = image_id\n\n        db.add_keypoints(image_id, keypoints)\n\n    return fname_to_id\n\ndef add_matches(db, h5_path, fname_to_id):\n    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n    \n    added = set()\n    n_keys = len(match_file.keys())\n    n_total = (n_keys * (n_keys - 1)) // 2\n\n    with tqdm(total=n_total) as pbar:\n        for key_1 in match_file.keys():\n            group = match_file[key_1]\n            for key_2 in group.keys():\n                id_1 = fname_to_id[key_1]\n                id_2 = fname_to_id[key_2]\n\n                pair_id = image_ids_to_pair_id(id_1, id_2)\n                if pair_id in added:\n                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n                    continue\n            \n                matches = group[key_2][()]\n                db.add_matches(id_1, id_2, matches)\n\n                added.add(pair_id)\n\n                pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:23.358072Z","iopub.execute_input":"2023-06-23T02:23:23.358487Z","iopub.status.idle":"2023-06-23T02:23:23.383774Z","shell.execute_reply.started":"2023-06-23T02:23:23.358434Z","shell.execute_reply":"2023-06-23T02:23:23.382581Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Making kornia local features loading w/o internet\nclass KeyNetAffNetHardNet(KF.LocalFeature):\n    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n\n    .. image:: _static/img/keynet_affnet.jpg\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int = 5000,\n        upright: bool = False,\n        device = torch.device('cpu'),\n        scale_laf: float = 1.0,\n    ):\n        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n        if not upright:\n            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n            ori_module.angle_detector.load_state_dict(weights)\n        detector = KF.KeyNetDetector(\n            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n        ).to(device)\n        kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n        detector.model.load_state_dict(kn_weights)\n        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n        detector.aff.load_state_dict(affnet_weights)\n        \n        hardnet = KF.HardNet(False).eval()\n        hn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n        hardnet.load_state_dict(hn_weights)\n        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n        super().__init__(detector, descriptor, scale_laf)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:27.256120Z","iopub.execute_input":"2023-06-23T02:23:27.256523Z","iopub.status.idle":"2023-06-23T02:23:27.268111Z","shell.execute_reply.started":"2023-06-23T02:23:27.256488Z","shell.execute_reply":"2023-06-23T02:23:27.266978Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def detect_features(img_fnames,\n                    num_feats = 2048,\n                    upright = False,\n                    device=torch.device('cpu'),\n                    feature_dir = '.featureout',\n                    resize_small_edge_to = 600):\n    if LOCAL_FEATURE == 'DISK':\n        # Load DISK from Kaggle models so it can run when the notebook is offline.\n        disk = KF.DISK().to(device)\n        pretrained_dict = torch.load('/kaggle/input/disk/pytorch/depth-supervision/1/loftr_outdoor.ckpt', map_location=device)\n        disk.load_state_dict(pretrained_dict['extractor'])\n        disk.eval()\n    if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n        feature = KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in progress_bar(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                timg = load_torch_image(img_path, device=device)\n                H, W = timg.shape[2:]\n                if resize_small_edge_to is None:\n                    timg_resized = timg\n                else:\n                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n                    print(f'Resized {timg.shape} to {timg_resized.shape} (resize_small_edge_to={resize_small_edge_to})')\n                h, w = timg_resized.shape[2:]\n                if LOCAL_FEATURE == 'DISK':\n                    features = disk(timg_resized, num_feats, pad_if_not_divisible=True)[0]\n                    kps1, descs = features.keypoints, features.descriptors\n                    \n                    lafs = KF.laf_from_center_scale_ori(kps1[None], torch.ones(1, len(kps1), 1, 1, device=device))\n                if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n                    lafs, resps, descs = feature(K.color.rgb_to_grayscale(timg_resized))\n                lafs[:,:,0,:] *= float(W) / float(w)\n                lafs[:,:,1,:] *= float(H) / float(h)\n                desc_dim = descs.shape[-1]\n                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n                f_laf[key] = lafs.detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return\n\ndef get_unique_idxs(A, dim=0):\n    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n    _, ind_sorted = torch.sort(idx, stable=True)\n    cum_sum = counts.cumsum(0)\n    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n    first_indices = ind_sorted[cum_sum]\n    return first_indices\n\ndef match_features(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout',\n                   device=torch.device('cpu'),\n                   min_matches=15, \n                   force_mutual = True,\n                   matching_alg='smnn'\n                  ):\n    assert matching_alg in ['smnn', 'adalam']\n    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n\n        for pair_idx in progress_bar(index_pairs):\n                    idx1, idx2 = pair_idx\n                    fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n                    key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n                    lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n                    lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n                    desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n                    desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n                    if matching_alg == 'adalam':\n                        img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n                        hw1, hw2 = img1.shape[:2], img2.shape[:2]\n                        adalam_config = KF.adalam.get_adalam_default_config()\n                        #adalam_config['orientation_difference_threshold'] = None\n                        #adalam_config['scale_rate_threshold'] = None\n                        adalam_config['force_seed_mnn']= False\n                        adalam_config['search_expansion'] = 16\n                        adalam_config['ransac_iters'] = 128\n                        adalam_config['device'] = device\n                        dists, idxs = KF.match_adalam(desc1, desc2,\n                                                      lafs1, lafs2, # Adalam takes into account also geometric information\n                                                      hw1=hw1, hw2=hw2,\n                                                      config=adalam_config) # Adalam also benefits from knowing image size\n                    else:\n                        dists, idxs = KF.match_smnn(desc1, desc2, 0.98)\n                    if len(idxs)  == 0:\n                        continue\n                    # Force mutual nearest neighbors\n                    if force_mutual:\n                        first_indices = get_unique_idxs(idxs[:,1])\n                        idxs = idxs[first_indices]\n                        dists = dists[first_indices]\n                    n_matches = len(idxs)\n                    if False:\n                        print (f'{key1}-{key2}: {n_matches} matches')\n                    group  = f_match.require_group(key1)\n                    if n_matches >= min_matches:\n                         group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n    return\n\ndef match_loftr(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout_loftr',\n                   device=torch.device('cpu'),\n                   min_matches=15, resize_to_ = (640, 480)):\n    matcher = KF.LoFTR(pretrained=None)\n    matcher.load_state_dict(torch.load('/kaggle/input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt')['state_dict'])\n    matcher = matcher.to(device).eval()\n\n    # First we do pairwise matching, and then extract \"keypoints\" from loftr matches.\n    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n        for pair_idx in progress_bar(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            # Load img1\n            timg1 = K.color.rgb_to_grayscale(load_torch_image(fname1, device=device))\n            H1, W1 = timg1.shape[2:]\n            if H1 < W1:\n                resize_to = resize_to_[1], resize_to_[0]\n            else:\n                resize_to = resize_to_\n            timg_resized1 = K.geometry.resize(timg1, resize_to, antialias=True)\n            h1, w1 = timg_resized1.shape[2:]\n\n            # Load img2\n            timg2 = K.color.rgb_to_grayscale(load_torch_image(fname2, device=device))\n            H2, W2 = timg2.shape[2:]\n            if H2 < W2:\n                resize_to2 = resize_to[1], resize_to[0]\n            else:\n                resize_to2 = resize_to_\n            timg_resized2 = K.geometry.resize(timg2, resize_to2, antialias=True)\n            h2, w2 = timg_resized2.shape[2:]\n            with torch.inference_mode():\n                input_dict = {\"image0\": timg_resized1,\"image1\": timg_resized2}\n                correspondences = matcher(input_dict)\n            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n\n            mkpts0[:,0] *= float(W1) / float(w1)\n            mkpts0[:,1] *= float(H1) / float(h1)\n\n            mkpts1[:,0] *= float(W2) / float(w2)\n            mkpts1[:,1] *= float(H2) / float(h2)\n\n            n_matches = len(mkpts1)\n            group  = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n\n    # Let's find unique loftr pixels and group them together.\n    kpts = defaultdict(list)\n    match_indexes = defaultdict(dict)\n    total_kpts=defaultdict(int)\n    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n        for k1 in f_match.keys():\n            group  = f_match[k1]\n            for k2 in group.keys():\n                matches = group[k2][...]\n                total_kpts[k1]\n                kpts[k1].append(matches[:, :2])\n                kpts[k2].append(matches[:, 2:])\n                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n                current_match[:, 0]+=total_kpts[k1]\n                current_match[:, 1]+=total_kpts[k2]\n                total_kpts[k1]+=len(matches)\n                total_kpts[k2]+=len(matches)\n                match_indexes[k1][k2]=current_match\n\n    for k in kpts.keys():\n        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n    unique_kpts = {}\n    unique_match_idxs = {}\n    out_match = defaultdict(dict)\n    for k in kpts.keys():\n        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n        unique_match_idxs[k] = uniq_reverse_idxs\n        unique_kpts[k] = uniq_kps.numpy()\n    for k1, group in match_indexes.items():\n        for k2, m in group.items():\n            m2 = deepcopy(m)\n            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n                                    unique_kpts[k2][  m2[:,1]],\n                                   ],\n                                   axis=1)\n            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n            m2_semiclean = m2[unique_idxs_current]\n            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n            m2_semiclean = m2_semiclean[unique_idxs_current1]\n            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n            out_match[k1][k2] = m2_semiclean2.numpy()\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n        for k, kpts1 in unique_kpts.items():\n            f_kp[k] = kpts1\n    \n    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for k1, gr in out_match.items():\n            group  = f_match.require_group(k1)\n            for k2, match in gr.items():\n                group[k2] = match\n    return\n\ndef import_into_colmap(img_dir,\n                       feature_dir ='.featureout',\n                       database_path = 'colmap.db',\n                       img_ext='.jpg'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, 'simple-radial', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n\n    db.commit()\n    return","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:28.616219Z","iopub.execute_input":"2023-06-23T02:23:28.616622Z","iopub.status.idle":"2023-06-23T02:23:28.678775Z","shell.execute_reply.started":"2023-06-23T02:23:28.616586Z","shell.execute_reply":"2023-06-23T02:23:28.677644Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2023'\n# src = '/kaggle/input/image-matching-challenge-2023/train'","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:33.355555Z","iopub.execute_input":"2023-06-23T02:23:33.355949Z","iopub.status.idle":"2023-06-23T02:23:33.360887Z","shell.execute_reply.started":"2023-06-23T02:23:33.355913Z","shell.execute_reply":"2023-06-23T02:23:33.359609Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Get data from csv.\n\ndata_dict = {}\n# with open(f'{src}/train_labels.csv', 'r') as f:\nwith open(f'{src}/sample_submission.csv', 'r') as f:\n\n    for i, l in enumerate(f):\n        # Skip header.\n        if l and i > 0:\n#             dataset, scene, image, _, _ = l.strip().split(',')\n            image, dataset, scene, _, _ = l.strip().split(',')\n\n            if dataset not in data_dict:\n                data_dict[dataset] = {}\n            if scene not in data_dict[dataset]:\n                data_dict[dataset][scene] = []\n            data_dict[dataset][scene].append(image)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:35.165125Z","iopub.execute_input":"2023-06-23T02:23:35.166186Z","iopub.status.idle":"2023-06-23T02:23:35.179630Z","shell.execute_reply.started":"2023-06-23T02:23:35.166135Z","shell.execute_reply":"2023-06-23T02:23:35.178626Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data_dict","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:38.803961Z","iopub.execute_input":"2023-06-23T02:23:38.804722Z","iopub.status.idle":"2023-06-23T02:23:38.813197Z","shell.execute_reply.started":"2023-06-23T02:23:38.804678Z","shell.execute_reply":"2023-06-23T02:23:38.811782Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'2cfa01ab573141e4': {'2fa124afd1f74f38': ['2cfa01ab573141e4/2fa124afd1f74f38/images/093870b65ebf4ded.png',\n   '2cfa01ab573141e4/2fa124afd1f74f38/images/11c073bfeee04544.png',\n   '2cfa01ab573141e4/2fa124afd1f74f38/images/11d5430cd9e246d4.png']}}"},"metadata":{}}]},{"cell_type":"code","source":"for dataset in data_dict:\n    for scene in data_dict[dataset]:\n        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:40.532418Z","iopub.execute_input":"2023-06-23T02:23:40.533079Z","iopub.status.idle":"2023-06-23T02:23:40.539138Z","shell.execute_reply.started":"2023-06-23T02:23:40.533033Z","shell.execute_reply":"2023-06-23T02:23:40.538011Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"2cfa01ab573141e4 / 2fa124afd1f74f38 -> 3 images\n","output_type":"stream"}]},{"cell_type":"code","source":"out_results = {}\ntimings = {\"shortlisting\":[],\n           \"feature_detection\": [],\n           \"feature_matching\":[],\n           \"RANSAC\": [],\n           \"Reconstruction\": []}","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:41.563027Z","iopub.execute_input":"2023-06-23T02:23:41.564053Z","iopub.status.idle":"2023-06-23T02:23:41.570060Z","shell.execute_reply.started":"2023-06-23T02:23:41.563959Z","shell.execute_reply":"2023-06-23T02:23:41.568874Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Function to create a submission file.\ndef create_submission(out_results, data_dict):\n    with open(f'submission.csv', 'w') as f:\n        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n        for dataset in data_dict:\n            if dataset in out_results:\n                res = out_results[dataset]\n            else:\n                res = {}\n            for scene in data_dict[dataset]:\n                if scene in res:\n                    scene_res = res[scene]\n                else:\n                    scene_res = {\"R\":{}, \"t\":{}}\n                for image in data_dict[dataset][scene]:\n                    if image in scene_res:\n                        print (image)\n                        R = scene_res[image]['R'].reshape(-1)\n                        T = scene_res[image]['t'].reshape(-1)\n                    else:\n                        R = np.eye(3).reshape(-1)\n                        T = np.zeros((3))\n                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:42.535941Z","iopub.execute_input":"2023-06-23T02:23:42.536690Z","iopub.status.idle":"2023-06-23T02:23:42.549588Z","shell.execute_reply.started":"2023-06-23T02:23:42.536650Z","shell.execute_reply":"2023-06-23T02:23:42.548443Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ndatasets = []\nfor dataset in data_dict:\n    datasets.append(dataset)\n\nfor dataset in datasets:\n    print(dataset)\n    if dataset not in out_results:\n        out_results[dataset] = {}\n    for scene in data_dict[dataset]:\n        print(scene)\n        # Fail gently if the notebook has not been submitted and the test data is not populated.\n        # You may want to run this on the training data in that case?\n        img_dir = f'{src}/test/{dataset}/{scene}/images'\n#         img_dir = f'{src}/{scene}'\n\n        if not os.path.exists(img_dir):\n            print(img_dir ,\"no images found\")\n            continue\n        # Wrap the meaty part in a try-except block.\n        try:\n            out_results[dataset][scene] = {}\n#                 img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n#             img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n            img_fnames = [f'{src}/test/{x}' for x in data_dict[dataset][scene]]\n\n            print (f\"Got {len(img_fnames)} images\")\n            feature_dir = f'featureout/{dataset}_{scene}'\n            if not os.path.isdir(feature_dir):\n                os.makedirs(feature_dir, exist_ok=True)\n            t=time()\n            index_pairs = get_image_pairs_shortlist(img_fnames,\n                                  sim_th = sim_th, # should be strict\n                                  min_pairs = min_pairs, # we select at least min_pairs PER IMAGE with biggest similarity\n                                  exhaustive_if_less = min_pairs,\n                                  device=device)\n            t=time() -t \n            timings['shortlisting'].append(t)\n            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n            gc.collect()\n            t=time()\n            if LOCAL_FEATURE != 'LoFTR':\n                detect_features(img_fnames, \n                                num_features,\n                                feature_dir=feature_dir,\n                                upright=True,\n                                device=device,\n                                resize_small_edge_to=resize_small_edge_to\n                               )\n                gc.collect()\n                t=time() -t \n                timings['feature_detection'].append(t)\n                print(f'Features detected in  {t:.4f} sec')\n                t=time()\n                match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device)\n#                 match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device, matching_alg='adalam')\n\n            else:\n                match_loftr(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n            t=time() -t \n            timings['feature_matching'].append(t)\n            print(f'Features matched in  {t:.4f} sec')\n            database_path = f'{feature_dir}/colmap.db'\n            if os.path.isfile(database_path):\n                os.remove(database_path)\n            gc.collect()\n            import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n            output_path = f'{feature_dir}/colmap_rec_{LOCAL_FEATURE}'\n\n            t=time()\n            pycolmap.match_exhaustive(database_path)\n            t=time() - t \n            timings['RANSAC'].append(t)\n            print(f'RANSAC in  {t:.4f} sec')\n\n            t=time()\n            # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n            mapper_options = pycolmap.IncrementalMapperOptions()\n            mapper_options.min_model_size = 3\n            os.makedirs(output_path, exist_ok=True)\n            maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n            print(maps)\n            #clear_output(wait=False)\n            t=time() - t\n            timings['Reconstruction'].append(t)\n            print(f'Reconstruction done in  {t:.4f} sec')\n            imgs_registered  = 0\n            best_idx = None\n            print (\"Looking for the best reconstruction\")\n            if isinstance(maps, dict):\n                idx_and_len = {}\n                for idx1, rec in maps.items():\n                    idx_and_len[idx1] = len(rec.images)\n\n                idx_and_len = dict(sorted(idx_and_len.items(), key=lambda item: item[1], reverse=True))\n                print(\"@@\", idx_and_len)\n    #             for idx1, rec in maps.items():\n    #                 print (idx1, rec.summary())\n    #                 if len(rec.images) > imgs_registered:\n    #                     imgs_registered = len(rec.images)\n    #                     best_idx = idx1\n            if len(idx_and_len) != 0:\n                assigned = []\n                for map_id, _ in idx_and_len.items():\n                    print (maps[map_id].summary())\n                    for k, im in maps[map_id].images.items():\n                        if im.name not in assigned:\n                            key1 = f'{dataset}/{scene}/images/{im.name}'\n                            out_results[dataset][scene][key1] = {}\n                            out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n                            out_results[dataset][scene][key1][\"t\"] = deepcopy(np.array(im.tvec))\n                            assigned.append(im.name)\n                        else:\n                            print(im.name, \"is already added.\")\n\n            print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n            print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n            create_submission(out_results, data_dict, out_name)\n            gc.collect()\n        except:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:43.741440Z","iopub.execute_input":"2023-06-23T02:23:43.741890Z","iopub.status.idle":"2023-06-23T02:23:43.980849Z","shell.execute_reply.started":"2023-06-23T02:23:43.741853Z","shell.execute_reply":"2023-06-23T02:23:43.979468Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"2cfa01ab573141e4\n2fa124afd1f74f38\n/kaggle/input/image-matching-challenge-2023/test/2cfa01ab573141e4/2fa124afd1f74f38/images no images found\n","output_type":"stream"}]},{"cell_type":"code","source":"# !tar cvf out.tar /kaggle/working\ncreate_submission(out_results, data_dict)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:46.721690Z","iopub.execute_input":"2023-06-23T02:23:46.722199Z","iopub.status.idle":"2023-06-23T02:23:46.728095Z","shell.execute_reply.started":"2023-06-23T02:23:46.722160Z","shell.execute_reply":"2023-06-23T02:23:46.726836Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# !head /kaggle/working/submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:48.735835Z","iopub.execute_input":"2023-06-23T02:23:48.736584Z","iopub.status.idle":"2023-06-23T02:23:48.741224Z","shell.execute_reply.started":"2023-06-23T02:23:48.736539Z","shell.execute_reply":"2023-06-23T02:23:48.740117Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# create_submission(out_results, data_dict)","metadata":{"execution":{"iopub.status.busy":"2023-06-23T02:23:50.933073Z","iopub.execute_input":"2023-06-23T02:23:50.934145Z","iopub.status.idle":"2023-06-23T02:23:50.938387Z","shell.execute_reply.started":"2023-06-23T02:23:50.934105Z","shell.execute_reply":"2023-06-23T02:23:50.937285Z"},"trusted":true},"execution_count":20,"outputs":[]}]}